{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron models\n",
    "Neuron models are featured heavily in machine learning as a way to convert data signals into some quantitative output. Neuron models are an algorithmic representation of biological neuron cells like those found in the human brain. Biological neurons are composed of dendrites, a cell body, and an axon. Chemical signals are received by the dendrites, creating an electrical potential in the neuron. If the electrical potential exceeds the threshold of a certain activation potential, then an electrical impulse is sent through the axon, which releases chemical signals to other neurons.\n",
    "\n",
    "Algorithm neuron models behave analogously. Instead of a chemical signal, a data signal--typically a vector--is received by the neuron. If the data signal produces a strong enough preactivation value, then the neuron produces an output value to the next neurons."
   ]
  },
  {
   "attachments": {
    "Artificial_neuron.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-neuron models\n",
    "The simplest neural network model is the single-neuron model. A neuron is defined by a set of \"preactivation weights\" $w \\in \\mathbb{R}^m$, a \"bias\" $b \\in \\mathbb{R}$, and an \"activation function\" $\\phi: \\mathbb{R}^{m+1} \\rightarrow \\mathbb{R}$. The model accepts a signal vector $x \\in \\mathbb{R}^m$. The preactivation function $w \\cdot x + b$ is computed. Then, the postactivation value $\\hat y = \\phi(w \\cdot x + b)$ is output by the neuron.\n",
    "\n",
    "![Artificial_neuron.png](attachment:Artificial_neuron.png)\n",
    "Image from [Wikipedia](https://en.wikipedia.org/wiki/File:Artificial_neuron.png).\n",
    "\n",
    "The neuron may be trained using an update method, which changes the neuron weights $w$ such that the neuron outputs become more optimal. There are many ways to update the weights and many ways to measure the optimality of neuron outputs. Our below (abstract) implementation of a single-neuron model chooses to train the neuron using the following scheme:\n",
    "\n",
    "1. Initialize $w \\in \\mathbb{R}^m, b \\in \\mathbb{R}$. These can be initialized randomly; we have chosen to initialize to all 0s.\n",
    "2. Repeat for a given number of epochs: Foreach data point $i$ composed of input signal $x_i \\in \\mathbb{R}^m$ and observation $y_i \\in \\mathbb{R}$:\n",
    "    1. Make model prediction $\\hat y_i = \\phi(w \\cdot x_i + b)$.\n",
    "    2. Update the weights $w, b$ using the input signal $x_i$, observation $y_i$, and prediction $\\hat y_i$.\n",
    "\n",
    "We have left the update method and the activation function as abstracted methods, as these methods will vary depending on neuron implementation and application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SingleNeuron(ABC):\n",
    "    '''\n",
    "    Abstract class for a single artificial neuron.\n",
    "    \n",
    "    ATTRIBUTES\n",
    "        weights    (m, 1)-shape array of weights in the preactivation function\n",
    "        bias       (1, 1)-shape array representing the model bias in the preactivation function\n",
    "    \n",
    "    METHODS\n",
    "        predict         Computes the neuron postactivation output from given input signal\n",
    "        train           Trains the neuron from given input signals and observations for a given number of epochs\n",
    "        activation      Abstract. Computes the postactivation value from the given the preactivation value\n",
    "        update          Abstract. Updates the neuron preactivation weights from the given data observation and neuron prediction\n",
    "        cost_function   Abstract. Computes the cost function from the given data observation and neuron prediction\n",
    "    '''\n",
    "    weights = None    # weights for preactivation function. 1-dimenional column array.\n",
    "    bias = None\n",
    "        \n",
    "    def predict(self, signal):\n",
    "        '''\n",
    "        Makes neuron prediction based on input signal (fires neuron).\n",
    "        Computes preactivation value using weights and input signal.\n",
    "        Then returns postactivation value by applying activation function to the preactivation value.\n",
    "        INPUT\n",
    "            signal; (n, m)-shape array of data signals. Each column represents a model feature (associated with weights). Each each row is a data point.\n",
    "        RETURNS\n",
    "            (n, 1)-shape vector of predictions, where each component is the prediction for a row from the input signal.\n",
    "        '''\n",
    "        preactivation = np.matmul(signal, self.weights) + self.bias\n",
    "        return self.activation(preactivation)\n",
    "\n",
    "    \n",
    "    def train(self, X_train, y_train, n_epoch):\n",
    "        '''\n",
    "        Trains the neuron for n_epoch epochs on training signals X_train and training observations y_train.\n",
    "        INPUT\n",
    "            X_train; (n, m)-shape array of data signals. Each column represents a model feature (associated with weights). Each each row is a data point.\n",
    "            y_train; (n, 1)-shape vector of data observations. Each component represents a separate data point. Parallel to X_train.\n",
    "            n_epoch; positive integer number of epochs to train\n",
    "        RETURNS\n",
    "            (n_epoch, )-shape vector, where each component i is the total cost function from epoch i \n",
    "        '''\n",
    "        # Get size of data input\n",
    "        n_sample, n_features = X_train.shape\n",
    "        \n",
    "        # Initialize weights and bias to 0s if they are not already initialized\n",
    "        if self.weights is None:\n",
    "            self.weights = np.zeros((n_features, 1))\n",
    "        if self.bias is None:\n",
    "            self.bias = 0\n",
    "            \n",
    "        # Create array to store each epoch's cost function\n",
    "        epoch_cost = np.zeros((n_epoch, ))\n",
    "        \n",
    "        # Train model by predicting each data point's observation based on the data point's input signal.\n",
    "        # Update the weights after each data point's prediction.\n",
    "        prog = tqdm(range(n_epoch)) # Progress bar\n",
    "        for epc in prog:\n",
    "            for ind in range(n_sample):\n",
    "                signal = X_train[ind, :].reshape((1, n_features))\n",
    "                predicted = self.predict(signal)[0]\n",
    "                actual = y_train[ind, 0]\n",
    "                self.update(actual, predicted, signal)\n",
    "                epoch_cost[epc] += self.cost_function(actual, predicted, signal)\n",
    "            prog.set_description(\"Cost: \" + str(round(epoch_cost[epc], 5)))\n",
    "            prog.update()\n",
    "        return epoch_cost\n",
    "    \n",
    "\n",
    "    @abstractmethod\n",
    "    def activation(self, preactivation):\n",
    "        '''\n",
    "        Given preactivation value(s), returns the postactivation value(s).\n",
    "        INPUT\n",
    "            preactivation; (n, 1)-shape vector, where component i is the preactivation value for some input signal i\n",
    "        RETURNS\n",
    "            (n, 1)-shape vector, where component i is the postactivation value for some input signal i\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self, actual, predicted, signal):\n",
    "        '''\n",
    "        Updates the neuron weights and bias.\n",
    "        INPUT\n",
    "            actual; scalar actual data observation y_i\n",
    "            predicted; scalar predicted observation yhat_i\n",
    "            signal; (1, m)-shape array of the data input signal x_i\n",
    "        '''\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def cost_function(self, actual, predicted, signal):\n",
    "        '''\n",
    "        Computes cost function for the given prediction.\n",
    "        INPUT\n",
    "            actual; scalar actual data observation y_i\n",
    "            predicted; scalar predicted observation yhat_i\n",
    "            signal; (1, m)-shape array of the data input signal x_i\n",
    "        '''\n",
    "        pass    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip_venv_inde577",
   "language": "python",
   "name": "pip_venv_inde577"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
